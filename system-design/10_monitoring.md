# Monitoring & Observability

Monitoring helps you understand what's happening in your system. Observability lets you debug complex distributed systems.

---

## ğŸ“Š Monitoring vs Observability

```
Monitoring: "Is the system working?"
  â€¢ Pre-defined metrics
  â€¢ Known failure modes
  â€¢ Dashboards and alerts
  â€¢ Example: CPU usage > 80%

Observability: "Why is the system broken?"
  â€¢ Explore unknown unknowns
  â€¢ Debug complex issues
  â€¢ Logs, metrics, traces
  â€¢ Example: Why is this specific request slow?
```

---

## ğŸ“ˆ The Three Pillars of Observability

### 1. Metrics (Numbers)

Time-series data: counters, gauges, histograms.

**Types of Metrics:**

```
Counter: Monotonically increasing
  â€¢ Total HTTP requests
  â€¢ Total errors
  â€¢ Total bytes transferred

Gauge: Can go up or down
  â€¢ Current CPU usage
  â€¢ Active connections
  â€¢ Queue depth

Histogram: Distribution of values
  â€¢ Request latency (p50, p95, p99)
  â€¢ Response sizes
  â€¢ Database query times
```

**Prometheus Example:**

```python
from prometheus_client import Counter, Gauge, Histogram, generate_latest

# Define metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

active_connections = Gauge(
    'active_connections',
    'Number of active database connections'
)

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

# Instrument code
@app.route('/api/users')
@request_duration.labels(method='GET', endpoint='/api/users').time()
def get_users():
    try:
        users = db.query("SELECT * FROM users")
        
        # Update metrics
        http_requests_total.labels(
            method='GET',
            endpoint='/api/users',
            status=200
        ).inc()
        
        active_connections.set(db.get_active_connections())
        
        return jsonify(users)
    
    except Exception as e:
        http_requests_total.labels(
            method='GET',
            endpoint='/api/users',
            status=500
        ).inc()
        raise

# Metrics endpoint (Prometheus scrapes this)
@app.route('/metrics')
def metrics():
    return generate_latest()
```

**Prometheus Query (PromQL):**

```promql
# Request rate (per second)
rate(http_requests_total[5m])

# Error rate
rate(http_requests_total{status="500"}[5m]) / rate(http_requests_total[5m])

# 95th percentile latency
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Requests by endpoint
sum(rate(http_requests_total[5m])) by (endpoint)
```

**Grafana Dashboard:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              System Overview Dashboard             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  [Request Rate]          [Error Rate]             â”‚
â”‚   1,234 req/s             0.5%                     â”‚
â”‚   â–² Chart                 â–² Chart                  â”‚
â”‚                                                    â”‚
â”‚  [Response Time (p95)]   [Active Connections]     â”‚
â”‚   250ms                   45                       â”‚
â”‚   â–² Chart                 â–² Chart                  â”‚
â”‚                                                    â”‚
â”‚  [CPU Usage]             [Memory Usage]            â”‚
â”‚   65%                     72%                      â”‚
â”‚   â–² Chart                 â–² Chart                  â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Logs (Events)

Structured records of what happened.

**Log Levels:**

```
DEBUG: Detailed diagnostic information
INFO: General informational messages
WARNING: Something unexpected, but system still works
ERROR: Error occurred, but system continues
CRITICAL: System failure, immediate action required
```

**Structured Logging (JSON):**

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_obj = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        # Add extra fields
        if hasattr(record, 'user_id'):
            log_obj['user_id'] = record.user_id
        if hasattr(record, 'request_id'):
            log_obj['request_id'] = record.request_id
        
        return json.dumps(log_obj)

# Configure logging
logger = logging.getLogger()
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Log with context
@app.route('/api/users/<user_id>')
def get_user(user_id):
    logger.info(
        'Fetching user',
        extra={'user_id': user_id, 'request_id': request.id}
    )
    
    try:
        user = db.get_user(user_id)
        
        logger.info(
            'User fetched successfully',
            extra={'user_id': user_id, 'request_id': request.id}
        )
        
        return jsonify(user)
    
    except Exception as e:
        logger.error(
            'Failed to fetch user',
            extra={
                'user_id': user_id,
                'request_id': request.id,
                'error': str(e)
            },
            exc_info=True
        )
        raise
```

**Log Output:**

```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "level": "INFO",
  "message": "Fetching user",
  "module": "api",
  "function": "get_user",
  "line": 45,
  "user_id": "123",
  "request_id": "abc-def-ghi"
}
```

**Centralized Logging (ELK Stack):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Service â”‚â”€â”€â”€â”€â”€â–¶â”‚Filebeat/ â”‚â”€â”€â”€â”€â”€â–¶â”‚Elasticsearch â”‚
â”‚   Logs   â”‚      â”‚Fluentd   â”‚      â”‚   (Store)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   Kibana     â”‚
                                    â”‚  (Visualize) â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Services write logs to stdout/files
2. Log shipper (Filebeat/Fluentd) collects logs
3. Elasticsearch stores and indexes logs
4. Kibana provides search and visualization
```

**Searching Logs (Kibana):**

```
# Find all errors for user 123
level: ERROR AND user_id: 123

# Find slow requests (> 1s)
duration: > 1000 AND endpoint: "/api/orders"

# Find errors in last 1 hour
level: ERROR AND timestamp: [now-1h TO now]
```

### 3. Traces (Distributed Tracing)

Track requests as they flow through multiple services.

**Distributed Trace:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Request: GET /api/orders/123                      â”‚
â”‚  Trace ID: abc123                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  [API Gateway] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 250ms        â”‚
â”‚    â”œâ”€ [Auth Service] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 20ms          â”‚
â”‚    â”œâ”€ [Order Service] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 200ms         â”‚
â”‚    â”‚   â”œâ”€ [Database Query] â”€â”€â”€â”€â”€â”€â”€ 150ms          â”‚
â”‚    â”‚   â””â”€ [Cache Check] â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5ms             â”‚
â”‚    â””â”€ [User Service] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 30ms          â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 250ms
Slowest: Order Service â†’ Database Query (150ms)
```

**OpenTelemetry Example:**

```python
from opentelemetry import trace
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Set up tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Export traces to Jaeger
jaeger_exporter = JaegerExporter(
    agent_host_name='localhost',
    agent_port=6831,
)
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

# Auto-instrument Flask
FlaskInstrumentor().instrument_app(app)

# Auto-instrument requests library
RequestsInstrumentor().instrument()

# Manual instrumentation
@app.route('/api/orders/<order_id>')
def get_order(order_id):
    with tracer.start_as_current_span("get_order") as span:
        span.set_attribute("order_id", order_id)
        
        # This span will be parent
        
        # Check cache (child span)
        with tracer.start_as_current_span("check_cache"):
            cached = redis.get(f"order:{order_id}")
            if cached:
                span.set_attribute("cache_hit", True)
                return jsonify(json.loads(cached))
        
        # Query database (child span)
        with tracer.start_as_current_span("database_query"):
            order = db.query(f"SELECT * FROM orders WHERE id = {order_id}")
            span.set_attribute("cache_hit", False)
        
        # Call user service (child span, auto-instrumented)
        user = requests.get(f'http://user-service/users/{order.user_id}')
        
        return jsonify(order)
```

**Trace Visualization (Jaeger):**

```
Request: GET /api/orders/123
Trace ID: abc123def456

Timeline:
0ms    50ms   100ms  150ms  200ms  250ms
â”‚â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”‚
â”‚                                         â”‚
â”œâ”€ get_order â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ 250ms
  â”‚                                       â”‚
  â”œâ”€ check_cache â”€â”¤ 5ms                   â”‚
  â”‚                                       â”‚
  â”œâ”€ database_query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚ 150ms
  â”‚                                       â”‚
  â””â”€ GET /users/123 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚ 50ms
```

---

## ğŸš¨ Alerting

Notify when something goes wrong.

### Alert Types

```
Threshold Alert:
  â€¢ CPU usage > 80% for 5 minutes
  â€¢ Error rate > 1% for 10 minutes
  
Anomaly Detection:
  â€¢ Request rate 3x higher than usual
  â€¢ Response time 2x slower than baseline
  
SLO Violation:
  â€¢ 99.9% availability not met this month
  â€¢ p95 latency > 500ms
```

### Prometheus Alerting Rules

```yaml
# prometheus-alerts.yml
groups:
  - name: example_alerts
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: rate(http_requests_total{status="500"}[5m]) > 0.05
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} (threshold: 0.05)"
      
      # High latency
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "p95 latency is {{ $value }}s (threshold: 1s)"
      
      # Service down
      - alert: ServiceDown
        expr: up{job="api-server"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down"
          description: "{{ $labels.instance }} is down"
```

### Alert Manager (Routing)

```yaml
# alertmanager.yml
route:
  receiver: team-pager
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  
  routes:
    # Critical alerts â†’ PagerDuty
    - match:
        severity: critical
      receiver: pagerduty
      continue: true
    
    # Warnings â†’ Slack
    - match:
        severity: warning
      receiver: slack

receivers:
  - name: pagerduty
    pagerduty_configs:
      - service_key: <pagerduty_key>
  
  - name: slack
    slack_configs:
      - api_url: https://hooks.slack.com/services/xxx
        channel: '#alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
```

---

## ğŸ” Health Checks

### Types of Health Checks

**1. Liveness Probe:** Is the service alive?

```python
@app.route('/health/live')
def liveness():
    # Simple check: is process running?
    return jsonify({'status': 'alive'}), 200
```

**2. Readiness Probe:** Is the service ready to serve traffic?

```python
@app.route('/health/ready')
def readiness():
    checks = {
        'database': check_database(),
        'redis': check_redis(),
        'external_api': check_external_api()
    }
    
    all_ready = all(checks.values())
    status_code = 200 if all_ready else 503
    
    return jsonify({
        'status': 'ready' if all_ready else 'not_ready',
        'checks': checks
    }), status_code

def check_database():
    try:
        db.execute("SELECT 1")
        return True
    except:
        return False

def check_redis():
    try:
        redis.ping()
        return True
    except:
        return False
```

**3. Startup Probe:** Has the service finished starting up?

```python
startup_complete = False

@app.route('/health/startup')
def startup():
    if startup_complete:
        return jsonify({'status': 'ready'}), 200
    else:
        return jsonify({'status': 'starting'}), 503

@app.before_first_request
def initialize():
    global startup_complete
    # Warm up caches, load data, etc.
    load_config()
    warm_cache()
    startup_complete = True
```

### Kubernetes Health Checks

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: api-server
spec:
  containers:
  - name: api
    image: api-server:latest
    
    # Liveness: Restart if fails
    livenessProbe:
      httpGet:
        path: /health/live
        port: 5000
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 3
    
    # Readiness: Remove from load balancer if fails
    readinessProbe:
      httpGet:
        path: /health/ready
        port: 5000
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 2
    
    # Startup: Wait for startup before other checks
    startupProbe:
      httpGet:
        path: /health/startup
        port: 5000
      failureThreshold: 30
      periodSeconds: 10
```

---

## ğŸ“‰ Debugging Distributed Systems

### Correlation IDs

Track requests across services:

```python
import uuid

@app.before_request
def add_correlation_id():
    # Get correlation ID from header or generate new
    correlation_id = request.headers.get('X-Correlation-ID', str(uuid.uuid4()))
    request.correlation_id = correlation_id

@app.after_request
def inject_correlation_id(response):
    response.headers['X-Correlation-ID'] = request.correlation_id
    return response

# Log with correlation ID
@app.route('/api/orders')
def create_order():
    logger.info(
        'Creating order',
        extra={'correlation_id': request.correlation_id}
    )
    
    # Pass correlation ID to downstream services
    response = requests.post(
        'http://inventory-service/reserve',
        headers={'X-Correlation-ID': request.correlation_id},
        json=order_data
    )
    
    return jsonify(order)
```

**Searching Logs by Correlation ID:**

```
# Find all logs for specific request
correlation_id: "abc-123-def-456"

Result:
  [API Gateway] Creating order (correlation_id: abc-123-def-456)
  [Inventory Service] Reserving items (correlation_id: abc-123-def-456)
  [Payment Service] Processing payment (correlation_id: abc-123-def-456)
  [API Gateway] Order created (correlation_id: abc-123-def-456)
```

### Error Tracking (Sentry)

```python
import sentry_sdk
from sentry_sdk.integrations.flask import FlaskIntegration

sentry_sdk.init(
    dsn="https://xxx@sentry.io/123",
    integrations=[FlaskIntegration()],
    traces_sample_rate=0.1,  # Sample 10% of traces
    environment="production"
)

@app.route('/api/process')
def process():
    try:
        # Business logic
        result = expensive_operation()
        return jsonify(result)
    
    except Exception as e:
        # Automatically captured by Sentry
        sentry_sdk.capture_exception(e)
        
        # Add context
        sentry_sdk.set_user({"id": request.user_id})
        sentry_sdk.set_context("request", {
            "endpoint": "/api/process",
            "method": "POST",
            "params": request.args
        })
        
        raise
```

**Sentry Dashboard:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Error: DatabaseConnectionError                    â”‚
â”‚  First seen: 2 hours ago                           â”‚
â”‚  Last seen: 5 minutes ago                          â”‚
â”‚  Occurrences: 234                                  â”‚
â”‚  Users affected: 45                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Stack Trace:                                      â”‚
â”‚    File "api.py", line 123, in process             â”‚
â”‚      result = db.query(sql)                        â”‚
â”‚    File "database.py", line 45, in query           â”‚
â”‚      conn = self.pool.get_connection()             â”‚
â”‚    DatabaseConnectionError: Connection timeout     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Context:                                          â”‚
â”‚    User: user-123                                  â”‚
â”‚    Endpoint: /api/process                          â”‚
â”‚    Request params: {filter: "active"}              â”‚
â”‚    Tags: {environment: "production"}               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š SLIs, SLOs, SLAs

### Service Level Indicators (SLIs)

Metrics that matter to users:

```
Availability:
  â€¢ Successful requests / Total requests
  â€¢ Example: 99,900 / 100,000 = 99.9%

Latency:
  â€¢ p50, p95, p99 response times
  â€¢ Example: p95 = 200ms

Error Rate:
  â€¢ Failed requests / Total requests
  â€¢ Example: 100 / 100,000 = 0.1%

Throughput:
  â€¢ Requests per second
  â€¢ Example: 1,000 req/s
```

### Service Level Objectives (SLOs)

Targets for SLIs:

```
Availability SLO:
  â€¢ 99.9% uptime (43.8 minutes downtime/month)
  
Latency SLO:
  â€¢ p95 response time < 500ms
  â€¢ p99 response time < 1s
  
Error Rate SLO:
  â€¢ < 0.1% error rate
```

### Service Level Agreements (SLAs)

Contractual commitments with customers:

```
SLA Example:
  â€¢ Uptime: 99.9% monthly
  â€¢ Latency: p95 < 1s
  â€¢ Support: 24/7 response within 1 hour
  â€¢ Penalty: 10% refund if SLA violated
```

**Error Budget:**

```
Availability SLO: 99.9%
Allowed downtime: 43.8 minutes/month

Tracking:
  â€¢ Day 1-10: 2 minutes downtime (4.6% of budget used)
  â€¢ Day 11-20: 40 minutes downtime (91.3% of budget used)
  â€¢ Day 21-30: 1.8 minutes left
  
Decision:
  â€¢ Budget exhausted? Freeze new features, focus on stability
  â€¢ Budget remaining? Continue feature development
```

---

## ğŸ¯ Interview Tips

**Key Points to Cover:**
1. âœ… Three pillars: Metrics, Logs, Traces
2. âœ… Difference between monitoring and observability
3. âœ… Health checks (liveness, readiness)
4. âœ… Alerting strategies (avoid alert fatigue)
5. âœ… SLOs and error budgets

**Common Questions:**
- "How would you debug a slow request?" â†’ Distributed tracing to find bottleneck
- "How to monitor microservices?" â†’ Centralized logging, metrics, traces with correlation IDs
- "What metrics would you track?" â†’ Request rate, error rate, latency (RED method)
- "How to avoid alert fatigue?" â†’ Set proper thresholds, group alerts, escalation policies
- "Difference between liveness and readiness?" â†’ Liveness: restart, Readiness: traffic routing

**RED Method (Monitoring):**
```
Rate: Requests per second
Error: Error rate (%)
Duration: Latency (p50, p95, p99)
```

**USE Method (Resources):**
```
Utilization: % time resource is busy
Saturation: Queue depth, wait time
Errors: Error count
```

---

**Next:** [Design Social Media Feed](12_social_feed.md)
